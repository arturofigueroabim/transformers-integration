{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "584bdac4-4c9f-4649-9478-960e9c118d0d",
   "metadata": {},
   "source": [
    "## ERNIE\n",
    "More details about ERNIE you can find them in this link:\n",
    "https://huggingface.co/docs/transformers/model_doc/ernie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47ae4fa4-faf8-4f90-affd-cb3e2c269f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_MODE\"] = \"dryrun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fb56783-60ae-4f9d-9a87-db0ef28c32f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import CONFIG\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from config import CONFIG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "109deb2f-235d-4bab-8506-c5ba5a1c2370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"78d69a339f5c9e47e83b23695b39e1f41fbe1fb3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "860207a9-c11b-47d2-929d-15a7c9421f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config Variables \n",
    "data_set= \"../data/kialo_references.pickle\" #\"../data/microtext_references.pickle\" \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96678dc2-86d7-437b-9081-e926ab77ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class RelationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        premise = self.data[\"premise\"].iloc[index]\n",
    "        claim = self.data[\"claim\"].iloc[index]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            premise,\n",
    "            claim,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "\n",
    "        if 'label' in self.data.columns:\n",
    "            \n",
    "            label = torch.tensor(0 if self.data[\"label\"].iloc[index] == \"Attack\" else 1, dtype=torch.int64)\n",
    "            \n",
    "            return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': label\n",
    "         }\n",
    "            \n",
    "        else:\n",
    "            return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "                    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88e7ae22-1fab-49bf-86cc-4532a4e295b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(mode: str, tokenizer, shuffle=False):\n",
    "    \n",
    "    df = pd.read_pickle(data_set)\n",
    "    split = df[df['mode'] == mode]\n",
    "    split = split[split['label'].isin(['Attack', 'Support'])]\n",
    "    #split.reset_index(drop=True)\n",
    "    \n",
    "    return RelationDataset(split, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2bb0760-e162-4cc5-8ced-a20cfa8d639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the datasets\n",
    "tokenizer = BertTokenizerFast.from_pretrained('nghuyong/ernie-2.0-large-en')\n",
    "train_dataset = create_dataset(\"train\", tokenizer, False)\n",
    "validate_dataset = create_dataset(\"validate\", tokenizer, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0456a51d-b60a-4b11-bb7e-7f11db1c706e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset class distribution:\n",
      " label\n",
      "Attack     89164\n",
      "Support    83416\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "df = pd.read_pickle(data_set)\n",
    "\n",
    "train_df = df[df['mode'] == 'train']\n",
    "labels = train_df[train_df['label'].isin(['Attack', 'Support'])]['label']\n",
    "print('Train dataset class distribution:\\n', labels.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12ca587a-1bc8-4826-a606-bb7cd186caf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights:  {'Attack': 0.9677672603292808, 'Support': 1.0344538218087658}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "                                        class_weight = \"balanced\",\n",
    "                                        classes = np.unique(labels),\n",
    "                                        y = labels                                                    \n",
    "                                    )\n",
    "dic_class_weights = dict(zip(np.unique(labels), class_weights))\n",
    "print(\"Class weights: \", dic_class_weights)\n",
    "\n",
    "# Convert the list to a tensor\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "class_weights = class_weights.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "192d0180-d3a3-42eb-8c47-e69fe980cd9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type ernie to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at nghuyong/ernie-2.0-large-en were not used when initializing BertForSequenceClassification: ['ernie.encoder.layer.1.attention.self.query.bias', 'ernie.encoder.layer.7.output.dense.weight', 'ernie.encoder.layer.19.output.LayerNorm.weight', 'ernie.encoder.layer.4.attention.output.dense.weight', 'ernie.encoder.layer.14.output.dense.bias', 'ernie.encoder.layer.1.attention.self.value.weight', 'ernie.encoder.layer.2.attention.output.dense.bias', 'ernie.encoder.layer.6.intermediate.dense.bias', 'ernie.encoder.layer.10.intermediate.dense.bias', 'ernie.encoder.layer.8.intermediate.dense.bias', 'ernie.encoder.layer.8.attention.output.LayerNorm.weight', 'ernie.encoder.layer.14.output.LayerNorm.bias', 'ernie.encoder.layer.18.intermediate.dense.bias', 'ernie.encoder.layer.4.attention.self.query.bias', 'ernie.encoder.layer.11.attention.self.query.bias', 'ernie.encoder.layer.7.attention.self.value.weight', 'ernie.encoder.layer.8.attention.self.value.weight', 'ernie.encoder.layer.21.intermediate.dense.weight', 'ernie.encoder.layer.1.output.dense.weight', 'ernie.encoder.layer.3.attention.output.dense.weight', 'ernie.encoder.layer.14.intermediate.dense.weight', 'ernie.embeddings.word_embeddings.weight', 'ernie.encoder.layer.23.output.LayerNorm.weight', 'ernie.encoder.layer.12.attention.self.key.bias', 'ernie.pooler.dense.bias', 'ernie.encoder.layer.19.attention.output.dense.weight', 'ernie.encoder.layer.13.attention.self.value.weight', 'ernie.encoder.layer.2.attention.output.LayerNorm.weight', 'ernie.encoder.layer.11.output.dense.weight', 'ernie.encoder.layer.16.output.dense.bias', 'ernie.encoder.layer.1.attention.output.LayerNorm.weight', 'ernie.encoder.layer.8.attention.self.value.bias', 'ernie.encoder.layer.11.attention.output.dense.weight', 'ernie.encoder.layer.4.intermediate.dense.weight', 'ernie.encoder.layer.14.attention.self.key.bias', 'ernie.encoder.layer.6.output.dense.bias', 'ernie.encoder.layer.5.attention.self.key.bias', 'ernie.encoder.layer.3.attention.self.key.bias', 'ernie.encoder.layer.7.output.LayerNorm.bias', 'ernie.encoder.layer.17.intermediate.dense.weight', 'ernie.encoder.layer.4.intermediate.dense.bias', 'ernie.encoder.layer.3.output.dense.weight', 'ernie.encoder.layer.1.attention.output.LayerNorm.bias', 'ernie.encoder.layer.5.attention.self.query.weight', 'ernie.encoder.layer.10.attention.self.key.weight', 'ernie.encoder.layer.3.attention.self.key.weight', 'ernie.encoder.layer.14.output.LayerNorm.weight', 'ernie.encoder.layer.2.output.dense.bias', 'ernie.encoder.layer.11.output.LayerNorm.bias', 'ernie.encoder.layer.22.attention.self.value.weight', 'ernie.encoder.layer.7.attention.output.LayerNorm.bias', 'ernie.encoder.layer.17.output.LayerNorm.weight', 'ernie.encoder.layer.9.attention.self.key.weight', 'ernie.encoder.layer.4.output.dense.weight', 'ernie.encoder.layer.21.attention.self.query.weight', 'ernie.encoder.layer.2.intermediate.dense.bias', 'ernie.encoder.layer.1.output.LayerNorm.weight', 'ernie.encoder.layer.4.output.LayerNorm.weight', 'ernie.encoder.layer.21.output.dense.weight', 'ernie.encoder.layer.7.attention.self.key.bias', 'ernie.encoder.layer.4.output.dense.bias', 'ernie.encoder.layer.15.attention.self.value.bias', 'ernie.encoder.layer.5.attention.output.dense.weight', 'ernie.encoder.layer.8.attention.output.LayerNorm.bias', 'ernie.encoder.layer.18.attention.self.key.weight', 'ernie.encoder.layer.1.output.LayerNorm.bias', 'ernie.encoder.layer.8.output.LayerNorm.bias', 'ernie.encoder.layer.11.attention.self.query.weight', 'ernie.encoder.layer.15.attention.self.query.bias', 'ernie.encoder.layer.0.attention.output.LayerNorm.bias', 'ernie.encoder.layer.8.output.LayerNorm.weight', 'ernie.encoder.layer.20.intermediate.dense.bias', 'ernie.encoder.layer.22.attention.output.dense.bias', 'ernie.encoder.layer.20.attention.self.value.weight', 'ernie.encoder.layer.10.output.dense.bias', 'ernie.encoder.layer.7.attention.self.value.bias', 'ernie.encoder.layer.13.output.dense.bias', 'ernie.encoder.layer.20.output.LayerNorm.bias', 'ernie.encoder.layer.15.output.dense.bias', 'ernie.encoder.layer.8.attention.output.dense.weight', 'ernie.encoder.layer.0.attention.self.query.bias', 'ernie.encoder.layer.10.intermediate.dense.weight', 'ernie.encoder.layer.17.attention.self.key.weight', 'ernie.encoder.layer.1.intermediate.dense.bias', 'ernie.encoder.layer.15.output.LayerNorm.bias', 'ernie.encoder.layer.5.attention.self.value.weight', 'ernie.encoder.layer.11.intermediate.dense.bias', 'ernie.encoder.layer.18.output.LayerNorm.bias', 'ernie.encoder.layer.7.attention.self.query.weight', 'ernie.encoder.layer.4.attention.output.LayerNorm.weight', 'ernie.encoder.layer.14.intermediate.dense.bias', 'ernie.encoder.layer.11.attention.self.key.weight', 'ernie.encoder.layer.0.attention.output.LayerNorm.weight', 'ernie.encoder.layer.0.attention.output.dense.weight', 'ernie.encoder.layer.19.intermediate.dense.weight', 'ernie.encoder.layer.14.attention.output.LayerNorm.bias', 'ernie.encoder.layer.21.attention.output.LayerNorm.weight', 'ernie.encoder.layer.14.attention.self.value.weight', 'ernie.encoder.layer.4.attention.self.key.bias', 'ernie.encoder.layer.5.intermediate.dense.bias', 'ernie.encoder.layer.12.attention.self.key.weight', 'ernie.encoder.layer.20.intermediate.dense.weight', 'ernie.encoder.layer.15.attention.output.LayerNorm.weight', 'ernie.encoder.layer.15.intermediate.dense.weight', 'ernie.encoder.layer.5.attention.self.query.bias', 'ernie.encoder.layer.23.intermediate.dense.weight', 'ernie.encoder.layer.17.attention.output.dense.weight', 'ernie.encoder.layer.23.attention.output.dense.bias', 'ernie.encoder.layer.13.attention.self.key.weight', 'ernie.encoder.layer.15.attention.output.LayerNorm.bias', 'ernie.encoder.layer.1.attention.self.key.weight', 'ernie.encoder.layer.5.intermediate.dense.weight', 'ernie.encoder.layer.17.output.dense.weight', 'ernie.encoder.layer.9.output.dense.bias', 'ernie.encoder.layer.12.attention.output.LayerNorm.weight', 'ernie.encoder.layer.13.attention.output.dense.weight', 'ernie.encoder.layer.11.attention.self.value.weight', 'ernie.encoder.layer.3.output.LayerNorm.weight', 'ernie.encoder.layer.4.attention.self.value.weight', 'ernie.encoder.layer.4.attention.self.key.weight', 'ernie.encoder.layer.3.attention.self.query.weight', 'ernie.encoder.layer.18.output.dense.bias', 'ernie.encoder.layer.6.output.dense.weight', 'ernie.encoder.layer.6.attention.self.key.bias', 'ernie.encoder.layer.2.attention.self.query.weight', 'ernie.encoder.layer.17.output.LayerNorm.bias', 'ernie.encoder.layer.10.attention.output.dense.weight', 'ernie.encoder.layer.10.output.LayerNorm.weight', 'ernie.encoder.layer.2.output.dense.weight', 'ernie.encoder.layer.15.attention.self.key.weight', 'ernie.encoder.layer.7.attention.output.dense.bias', 'ernie.encoder.layer.22.attention.self.key.bias', 'ernie.encoder.layer.13.output.LayerNorm.bias', 'ernie.encoder.layer.16.attention.output.dense.bias', 'ernie.encoder.layer.16.attention.self.query.weight', 'ernie.encoder.layer.22.output.LayerNorm.bias', 'ernie.encoder.layer.22.attention.self.key.weight', 'ernie.encoder.layer.14.attention.self.query.bias', 'ernie.encoder.layer.23.attention.self.value.weight', 'ernie.encoder.layer.13.attention.output.dense.bias', 'ernie.encoder.layer.18.attention.self.value.weight', 'ernie.encoder.layer.18.output.dense.weight', 'ernie.encoder.layer.4.attention.output.dense.bias', 'ernie.encoder.layer.7.output.LayerNorm.weight', 'ernie.encoder.layer.22.output.LayerNorm.weight', 'ernie.encoder.layer.19.attention.self.query.weight', 'ernie.encoder.layer.5.attention.self.key.weight', 'ernie.encoder.layer.6.intermediate.dense.weight', 'ernie.encoder.layer.23.output.dense.weight', 'ernie.encoder.layer.6.attention.self.key.weight', 'ernie.encoder.layer.9.attention.self.key.bias', 'ernie.encoder.layer.7.intermediate.dense.bias', 'ernie.encoder.layer.21.attention.self.query.bias', 'ernie.encoder.layer.16.attention.output.LayerNorm.weight', 'ernie.encoder.layer.2.attention.self.key.weight', 'ernie.encoder.layer.4.output.LayerNorm.bias', 'ernie.encoder.layer.16.intermediate.dense.weight', 'ernie.encoder.layer.14.attention.output.LayerNorm.weight', 'ernie.encoder.layer.0.attention.self.value.bias', 'ernie.encoder.layer.3.output.dense.bias', 'ernie.encoder.layer.4.attention.self.value.bias', 'ernie.encoder.layer.9.attention.self.value.weight', 'ernie.encoder.layer.15.attention.self.key.bias', 'ernie.encoder.layer.5.attention.self.value.bias', 'ernie.encoder.layer.18.attention.output.LayerNorm.bias', 'ernie.encoder.layer.3.attention.output.LayerNorm.bias', 'ernie.encoder.layer.17.attention.output.LayerNorm.weight', 'ernie.encoder.layer.21.attention.self.key.weight', 'ernie.encoder.layer.10.attention.self.value.weight', 'ernie.encoder.layer.11.attention.output.LayerNorm.bias', 'ernie.encoder.layer.2.attention.self.value.weight', 'ernie.encoder.layer.16.intermediate.dense.bias', 'ernie.encoder.layer.0.intermediate.dense.bias', 'ernie.encoder.layer.17.attention.output.dense.bias', 'ernie.encoder.layer.4.attention.output.LayerNorm.bias', 'ernie.encoder.layer.9.attention.output.dense.bias', 'ernie.encoder.layer.0.output.dense.bias', 'ernie.encoder.layer.0.attention.self.key.bias', 'ernie.encoder.layer.2.attention.self.query.bias', 'ernie.encoder.layer.21.attention.output.dense.weight', 'ernie.encoder.layer.13.attention.self.query.weight', 'ernie.encoder.layer.0.output.LayerNorm.weight', 'ernie.embeddings.position_embeddings.weight', 'ernie.encoder.layer.23.output.dense.bias', 'ernie.encoder.layer.21.intermediate.dense.bias', 'ernie.encoder.layer.8.attention.self.query.bias', 'ernie.encoder.layer.1.intermediate.dense.weight', 'ernie.encoder.layer.1.attention.output.dense.weight', 'ernie.encoder.layer.20.attention.output.dense.bias', 'ernie.encoder.layer.22.attention.output.LayerNorm.bias', 'ernie.encoder.layer.8.attention.self.query.weight', 'ernie.encoder.layer.16.attention.self.key.weight', 'ernie.encoder.layer.20.output.dense.weight', 'ernie.encoder.layer.5.output.LayerNorm.weight', 'ernie.encoder.layer.18.intermediate.dense.weight', 'ernie.encoder.layer.23.attention.self.key.weight', 'ernie.encoder.layer.20.output.LayerNorm.weight', 'ernie.encoder.layer.15.output.LayerNorm.weight', 'ernie.encoder.layer.16.output.LayerNorm.bias', 'ernie.encoder.layer.20.attention.output.LayerNorm.bias', 'ernie.encoder.layer.19.output.dense.bias', 'ernie.encoder.layer.23.attention.self.key.bias', 'ernie.encoder.layer.19.output.LayerNorm.bias', 'ernie.encoder.layer.19.attention.self.value.weight', 'ernie.encoder.layer.2.attention.self.value.bias', 'ernie.encoder.layer.12.output.dense.bias', 'ernie.encoder.layer.6.attention.self.query.bias', 'ernie.encoder.layer.2.attention.output.dense.weight', 'ernie.encoder.layer.18.output.LayerNorm.weight', 'ernie.encoder.layer.13.output.dense.weight', 'ernie.encoder.layer.12.attention.self.query.weight', 'ernie.encoder.layer.8.output.dense.weight', 'ernie.encoder.layer.6.attention.output.dense.bias', 'ernie.encoder.layer.9.attention.output.LayerNorm.bias', 'ernie.encoder.layer.21.output.dense.bias', 'ernie.encoder.layer.21.output.LayerNorm.weight', 'ernie.encoder.layer.11.attention.output.LayerNorm.weight', 'ernie.encoder.layer.3.attention.output.LayerNorm.weight', 'ernie.encoder.layer.19.intermediate.dense.bias', 'ernie.encoder.layer.17.attention.self.value.bias', 'ernie.encoder.layer.14.attention.self.query.weight', 'ernie.encoder.layer.3.intermediate.dense.bias', 'ernie.encoder.layer.2.output.LayerNorm.weight', 'ernie.encoder.layer.9.attention.output.LayerNorm.weight', 'ernie.encoder.layer.20.attention.self.query.bias', 'ernie.encoder.layer.17.attention.self.key.bias', 'ernie.encoder.layer.10.attention.output.LayerNorm.bias', 'ernie.encoder.layer.3.attention.self.value.bias', 'ernie.encoder.layer.3.attention.self.value.weight', 'ernie.encoder.layer.13.attention.output.LayerNorm.bias', 'ernie.embeddings.LayerNorm.bias', 'ernie.encoder.layer.14.output.dense.weight', 'ernie.encoder.layer.12.attention.self.query.bias', 'ernie.encoder.layer.3.output.LayerNorm.bias', 'ernie.encoder.layer.15.attention.output.dense.bias', 'ernie.encoder.layer.12.output.LayerNorm.weight', 'ernie.encoder.layer.2.output.LayerNorm.bias', 'ernie.encoder.layer.16.output.LayerNorm.weight', 'ernie.encoder.layer.15.output.dense.weight', 'ernie.encoder.layer.20.attention.self.query.weight', 'ernie.encoder.layer.0.output.dense.weight', 'ernie.encoder.layer.3.attention.self.query.bias', 'ernie.encoder.layer.12.intermediate.dense.bias', 'ernie.encoder.layer.0.attention.output.dense.bias', 'ernie.encoder.layer.21.attention.output.LayerNorm.bias', 'ernie.encoder.layer.20.attention.output.dense.weight', 'ernie.encoder.layer.7.intermediate.dense.weight', 'ernie.encoder.layer.12.attention.self.value.bias', 'ernie.encoder.layer.18.attention.output.dense.bias', 'ernie.encoder.layer.10.attention.self.query.bias', 'ernie.encoder.layer.1.attention.self.query.weight', 'ernie.encoder.layer.13.attention.self.value.bias', 'ernie.encoder.layer.6.attention.output.LayerNorm.weight', 'ernie.encoder.layer.12.attention.self.value.weight', 'ernie.encoder.layer.19.attention.self.value.bias', 'ernie.encoder.layer.2.intermediate.dense.weight', 'ernie.encoder.layer.18.attention.self.key.bias', 'ernie.encoder.layer.20.output.dense.bias', 'ernie.encoder.layer.11.attention.output.dense.bias', 'ernie.encoder.layer.15.intermediate.dense.bias', 'ernie.encoder.layer.7.output.dense.bias', 'ernie.encoder.layer.0.attention.self.value.weight', 'ernie.encoder.layer.8.attention.output.dense.bias', 'ernie.encoder.layer.10.attention.output.dense.bias', 'ernie.encoder.layer.8.intermediate.dense.weight', 'ernie.encoder.layer.16.output.dense.weight', 'ernie.encoder.layer.16.attention.output.dense.weight', 'ernie.encoder.layer.9.attention.self.query.bias', 'ernie.encoder.layer.6.attention.self.query.weight', 'ernie.encoder.layer.23.attention.output.dense.weight', 'ernie.encoder.layer.12.attention.output.dense.bias', 'ernie.encoder.layer.3.intermediate.dense.weight', 'ernie.encoder.layer.4.attention.self.query.weight', 'ernie.encoder.layer.18.attention.output.LayerNorm.weight', 'ernie.encoder.layer.22.attention.self.query.weight', 'ernie.encoder.layer.6.attention.output.LayerNorm.bias', 'ernie.encoder.layer.13.output.LayerNorm.weight', 'ernie.encoder.layer.15.attention.output.dense.weight', 'ernie.encoder.layer.7.attention.output.dense.weight', 'ernie.encoder.layer.17.attention.self.query.bias', 'ernie.encoder.layer.17.intermediate.dense.bias', 'ernie.encoder.layer.1.output.dense.bias', 'ernie.encoder.layer.13.attention.self.key.bias', 'ernie.encoder.layer.14.attention.output.dense.bias', 'ernie.encoder.layer.18.attention.output.dense.weight', 'ernie.encoder.layer.16.attention.output.LayerNorm.bias', 'ernie.encoder.layer.22.attention.output.dense.weight', 'ernie.encoder.layer.2.attention.self.key.bias', 'ernie.encoder.layer.5.output.LayerNorm.bias', 'ernie.encoder.layer.13.intermediate.dense.bias', 'ernie.encoder.layer.16.attention.self.query.bias', 'ernie.encoder.layer.19.attention.self.key.bias', 'ernie.encoder.layer.2.attention.output.LayerNorm.bias', 'ernie.encoder.layer.19.attention.self.query.bias', 'ernie.encoder.layer.6.attention.self.value.weight', 'ernie.encoder.layer.0.output.LayerNorm.bias', 'ernie.encoder.layer.21.attention.self.value.weight', 'ernie.encoder.layer.8.attention.self.key.bias', 'ernie.encoder.layer.5.attention.output.LayerNorm.weight', 'ernie.encoder.layer.10.output.dense.weight', 'ernie.encoder.layer.12.attention.output.LayerNorm.bias', 'ernie.encoder.layer.11.output.dense.bias', 'ernie.encoder.layer.17.output.dense.bias', 'ernie.encoder.layer.5.attention.output.dense.bias', 'ernie.encoder.layer.15.attention.self.query.weight', 'ernie.encoder.layer.12.attention.output.dense.weight', 'ernie.encoder.layer.23.attention.self.query.weight', 'ernie.encoder.layer.10.attention.output.LayerNorm.weight', 'ernie.encoder.layer.20.attention.self.key.weight', 'ernie.encoder.layer.23.attention.output.LayerNorm.weight', 'ernie.encoder.layer.1.attention.self.key.bias', 'ernie.encoder.layer.19.attention.output.LayerNorm.weight', 'ernie.encoder.layer.10.attention.self.key.bias', 'ernie.encoder.layer.9.output.dense.weight', 'ernie.encoder.layer.22.attention.self.query.bias', 'ernie.encoder.layer.20.attention.output.LayerNorm.weight', 'ernie.encoder.layer.23.attention.self.query.bias', 'ernie.encoder.layer.15.attention.self.value.weight', 'ernie.encoder.layer.1.attention.output.dense.bias', 'ernie.encoder.layer.19.attention.output.dense.bias', 'ernie.encoder.layer.17.attention.self.value.weight', 'ernie.encoder.layer.6.attention.output.dense.weight', 'ernie.encoder.layer.21.output.LayerNorm.bias', 'ernie.embeddings.token_type_embeddings.weight', 'ernie.encoder.layer.23.intermediate.dense.bias', 'ernie.encoder.layer.13.intermediate.dense.weight', 'ernie.encoder.layer.9.attention.output.dense.weight', 'ernie.encoder.layer.18.attention.self.query.bias', 'ernie.encoder.layer.19.attention.self.key.weight', 'ernie.encoder.layer.11.attention.self.value.bias', 'ernie.encoder.layer.5.output.dense.bias', 'ernie.encoder.layer.16.attention.self.key.bias', 'ernie.encoder.layer.14.attention.self.key.weight', 'ernie.encoder.layer.13.attention.self.query.bias', 'ernie.encoder.layer.0.attention.self.query.weight', 'ernie.encoder.layer.9.intermediate.dense.weight', 'ernie.encoder.layer.11.intermediate.dense.weight', 'ernie.encoder.layer.16.attention.self.value.weight', 'ernie.encoder.layer.14.attention.self.value.bias', 'ernie.encoder.layer.21.attention.self.value.bias', 'ernie.encoder.layer.9.attention.self.query.weight', 'ernie.encoder.layer.16.attention.self.value.bias', 'ernie.encoder.layer.5.output.dense.weight', 'ernie.encoder.layer.6.output.LayerNorm.weight', 'ernie.encoder.layer.10.attention.self.query.weight', 'ernie.encoder.layer.0.intermediate.dense.weight', 'ernie.encoder.layer.14.attention.output.dense.weight', 'ernie.encoder.layer.23.output.LayerNorm.bias', 'ernie.embeddings.LayerNorm.weight', 'ernie.encoder.layer.7.attention.self.query.bias', 'ernie.encoder.layer.8.attention.self.key.weight', 'ernie.encoder.layer.8.output.dense.bias', 'ernie.encoder.layer.22.output.dense.bias', 'ernie.encoder.layer.9.output.LayerNorm.weight', 'ernie.encoder.layer.22.attention.output.LayerNorm.weight', 'ernie.encoder.layer.12.intermediate.dense.weight', 'ernie.encoder.layer.19.output.dense.weight', 'ernie.encoder.layer.7.attention.output.LayerNorm.weight', 'ernie.encoder.layer.21.attention.self.key.bias', 'ernie.encoder.layer.20.attention.self.key.bias', 'ernie.encoder.layer.18.attention.self.query.weight', 'ernie.encoder.layer.9.intermediate.dense.bias', 'ernie.encoder.layer.17.attention.self.query.weight', 'ernie.encoder.layer.9.attention.self.value.bias', 'ernie.encoder.layer.7.attention.self.key.weight', 'ernie.encoder.layer.5.attention.output.LayerNorm.bias', 'ernie.encoder.layer.23.attention.self.value.bias', 'ernie.encoder.layer.10.output.LayerNorm.bias', 'ernie.encoder.layer.20.attention.self.value.bias', 'ernie.encoder.layer.23.attention.output.LayerNorm.bias', 'ernie.encoder.layer.9.output.LayerNorm.bias', 'ernie.encoder.layer.0.attention.self.key.weight', 'ernie.encoder.layer.19.attention.output.LayerNorm.bias', 'ernie.encoder.layer.10.attention.self.value.bias', 'ernie.encoder.layer.6.attention.self.value.bias', 'ernie.encoder.layer.22.attention.self.value.bias', 'ernie.encoder.layer.22.output.dense.weight', 'ernie.encoder.layer.12.output.dense.weight', 'ernie.encoder.layer.3.attention.output.dense.bias', 'ernie.encoder.layer.1.attention.self.value.bias', 'ernie.encoder.layer.17.attention.output.LayerNorm.bias', 'ernie.encoder.layer.21.attention.output.dense.bias', 'ernie.encoder.layer.18.attention.self.value.bias', 'ernie.pooler.dense.weight', 'ernie.encoder.layer.22.intermediate.dense.weight', 'ernie.encoder.layer.12.output.LayerNorm.bias', 'ernie.encoder.layer.11.output.LayerNorm.weight', 'ernie.encoder.layer.13.attention.output.LayerNorm.weight', 'ernie.encoder.layer.6.output.LayerNorm.bias', 'ernie.encoder.layer.22.intermediate.dense.bias', 'ernie.encoder.layer.11.attention.self.key.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nghuyong/ernie-2.0-large-en and are newly initialized: ['encoder.layer.0.attention.self.value.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.12.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.15.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.14.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.12.output.dense.weight', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.23.attention.self.value.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.20.output.dense.bias', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.12.intermediate.dense.bias', 'classifier.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.15.output.dense.bias', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.19.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.21.output.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.14.output.dense.bias', 'encoder.layer.16.output.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.22.output.dense.bias', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.9.attention.self.query.bias', 'pooler.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.21.output.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.9.intermediate.dense.bias', 'pooler.dense.bias', 'encoder.layer.13.output.dense.weight', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.18.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.23.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.16.output.dense.weight', 'encoder.layer.20.output.dense.weight', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.17.intermediate.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.21.intermediate.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.17.output.dense.bias', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.18.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.23.output.dense.weight', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.19.output.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.13.output.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.22.output.dense.weight', 'encoder.layer.12.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.17.output.dense.weight', 'classifier.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.10.attention.output.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained ERNIE model for sequence classification\n",
    "ernie_model = BertForSequenceClassification.from_pretrained('nghuyong/ernie-2.0-large-en', \n",
    "                                                            num_labels=2, \n",
    "                                                            output_attentions = False, \n",
    "                                                            output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a5a0839d-a1af-44db-85b0-11936df40882",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afigueroa/tfIntegration/.venv/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='365' max='365' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [365/365 00:59, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.684600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.871800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.823500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.077900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.877700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.341800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.427800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.789600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.029900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.815600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.157100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.782000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.733100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.718200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.751500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.760600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.772800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.819600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.878500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.689900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.737300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.831600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.718500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.710200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.646200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.724000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.706800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.705200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.671100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.714000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.663200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.722300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.656800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.751900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.722900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=365, training_loss=0.8089660997260106, metrics={'train_runtime': 59.8257, 'train_samples_per_second': 24.154, 'train_steps_per_second': 6.101, 'total_flos': 336660205002240.0, 'train_loss': 0.8089660997260106, 'epoch': 5.0})"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "    \n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, model, args, train_dataset, loss_fct, **kwargs):\n",
    "        super().__init__(model, args, train_dataset=train_dataset, **kwargs)\n",
    "        self.loss_fct = loss_fct\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = self.loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Define your loss function\n",
    "loss_fct = CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,              # total number of training epochs\n",
    "    per_device_train_batch_size=4,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=100,                # number of warmup steps for the learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    learning_rate=3e-5,\n",
    "    #logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Initialize your trainer with your custom loss function\n",
    "trainer = CustomTrainer(\n",
    "    model=ernie_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validate_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    loss_fct=loss_fct,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b890a0f7-6709-420f-abfa-a08e13cb4792",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afigueroa/tfIntegration/.venv/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.617200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.595500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.594800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.559600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.590300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=0.5914792919158935, metrics={'train_runtime': 35.5472, 'train_samples_per_second': 41.916, 'train_steps_per_second': 1.407, 'total_flos': 347144432839680.0, 'train_loss': 0.5914792919158935, 'epoch': 5.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,              # total number of training epochs\n",
    "    per_device_train_batch_size=32,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for the learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    learning_rate=3e-5,\n",
    "    #logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Create the Trainer and train\n",
    "trainer = Trainer(\n",
    "    model=ernie_model,               # the instantiated Transformers model to be trained\n",
    "    args=training_args,              # training arguments, defined above\n",
    "    train_dataset=train_dataset,     # training dataset\n",
    "    #eval_dataset=validate_dataset,   # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7ec619f0-f79b-4926-931d-124b4dd03501",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64/3032235913.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  split['label'] = split['label'].map(mapping)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        15\n",
      "           1       0.69      1.00      0.82        34\n",
      "\n",
      "    accuracy                           0.69        49\n",
      "   macro avg       0.35      0.50      0.41        49\n",
      "weighted avg       0.48      0.69      0.57        49\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afigueroa/tfIntegration/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/afigueroa/tfIntegration/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/afigueroa/tfIntegration/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the test dataset\n",
    "mapping = {'Attack': 0, 'Support': 1}\n",
    "df = pd.read_pickle(data_set)\n",
    "split = df[df['mode'] == 'test']\n",
    "split['label'] = split['label'].map(mapping)\n",
    "\n",
    "test_dataset = RelationDataset(split, tokenizer)\n",
    "\n",
    "# Make predictions\n",
    "raw_pred, _, _ = trainer.predict(test_dataset)\n",
    "preds = raw_pred.argmax(axis=1)\n",
    "\n",
    "# Print classification report\n",
    "report = classification_report(split['label'].values, preds)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8e5b222c-6ac9-4136-a2d9-507cbeac1606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f95f1a2e-f0fa-42ac-aa2e-e939b0c13da1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6070c6de-9218-4284-85b9-f33d158069f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "trainer.save_model(\"./models/kialo\")\n",
    "tokenizer.save_pretrained(\"./models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a7fe009c-9d87-49b7-97ef-32ff576e8661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8110434412956238,\n",
       " 'eval_accuracy': 0.6170212765957447,\n",
       " 'eval_f1': 0.75,\n",
       " 'eval_precision': 0.675,\n",
       " 'eval_recall': 0.84375,\n",
       " 'eval_runtime': 0.4056,\n",
       " 'eval_samples_per_second': 115.879,\n",
       " 'eval_steps_per_second': 2.466,\n",
       " 'epoch': 8.0}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c918cf2-825a-4e9d-b6c5-87380069055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ERNIE\n",
    "More details about ERNIE you can find them in this link:\n",
    "https://huggingface.co/docs/transformers/model_doc/ernie\n",
    "\n",
    "# Program\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class RelationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        premise = self.data[\"premise\"].iloc[index]\n",
    "        claim = self.data[\"claim\"].iloc[index]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            premise,\n",
    "            claim,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "\n",
    "        if 'label' in self.data.columns:\n",
    "            \n",
    "            label = torch.tensor(0 if self.data[\"label\"].iloc[index] == \"Attack\" else 1, dtype=torch.int64)\n",
    "            \n",
    "            return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': label\n",
    "         }\n",
    "            \n",
    "        else:\n",
    "            return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "                    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def create_dataset(mode: str, tokenizer, shuffle=False):\n",
    "    \n",
    "    df = pd.read_pickle(data_set)\n",
    "    split = df[df['mode'] == mode]\n",
    "    split = split[split['label'] != 'Support: Example']\n",
    "    #split.reset_index(drop=True)\n",
    "    \n",
    "    return RelationDataset(split, tokenizer)\n",
    "    \n",
    "#Create the datasets\n",
    "tokenizer = BertTokenizerFast.from_pretrained('nghuyong/ernie-2.0-large-en')\n",
    "train_dataset = create_dataset(\"train\", tokenizer, False)\n",
    "validate_dataset = create_dataset(\"validate\", tokenizer, False)\n",
    "\n",
    "# Load datasets\n",
    "df = pd.read_pickle(data_set)\n",
    "\n",
    "train_df = df[df['mode'] == 'train']\n",
    "labels = train_df[train_df['label'] != 'Support: Example']['label']\n",
    "print('Train dataset class distribution:\\n', labels.value_counts())\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "                                        class_weight = \"balanced\",\n",
    "                                        classes = np.unique(labels),\n",
    "                                        y = labels                                                    \n",
    "                                    )\n",
    "dic_class_weights = dict(zip(np.unique(labels), class_weights))\n",
    "print(\"Class weights: \", dic_class_weights)\n",
    "\n",
    "# Convert the list to a tensor\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "# Load the pre-trained ERNIE model for sequence classification\n",
    "ernie_model = BertForSequenceClassification.from_pretrained('nghuyong/ernie-2.0-large-en', \n",
    "                                                            num_labels=2, \n",
    "                                                            output_attentions = False, \n",
    "                                                            output_hidden_states = False)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "    \n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, model, args, train_dataset, loss_fct, **kwargs):\n",
    "        super().__init__(model, args, train_dataset=train_dataset, **kwargs)\n",
    "        self.loss_fct = loss_fct\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = self.loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Define your loss function\n",
    "loss_fct = CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,              # total number of training epochs\n",
    "    per_device_train_batch_size=4,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=100,                # number of warmup steps for the learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    learning_rate=3e-5,\n",
    "    #logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Initialize your trainer with your custom loss function\n",
    "trainer = CustomTrainer(\n",
    "    model=ernie_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validate_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    loss_fct=loss_fct,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
