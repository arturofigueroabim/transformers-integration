{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2578b0eb-8ed9-4a49-b0d7-154bb978ea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "from transformers import RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from config import CONFIG\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e105d9a-c756-482d-8eed-ee1627652f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: generalize to [String] -> [Class] system \n",
    "# TODO: generalize forward pass\n",
    "# TODO: set parameter that enables cls token utilization or arbitrary hidden layer utilization\n",
    "# TODO: (MAYBE) generalize models to extend BASE, otherwise add \n",
    "from config import CONFIG\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from torch import nn\n",
    "\n",
    "class BaseClassificationModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, dropout = 0.05, n_classes = 2, injection = False):\n",
    "        super(BaseClassificationModel, self).__init__()\n",
    "        \n",
    "        \n",
    "        # model body\n",
    "        self.model = AutoModel.from_pretrained(CONFIG.pretrained_model_name)\n",
    "        \n",
    "        self.hidden_size = self.model.config.hidden_size #768\n",
    "        \n",
    "        # (standard) model classification head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.hidden_size, n_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # initialize weights in linear layers\n",
    "        self.init_weights(self.head)\n",
    "        \n",
    "        \n",
    "    def init_weights(self, module):\n",
    "        for layer in module:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.weight.data.normal_(mean = 0.0, std = 0.02)\n",
    "                if layer.bias is not None:\n",
    "                    layer.bias.data.zero_()\n",
    "                    \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        \n",
    "        output = self.model(input_ids = input_ids, \n",
    "                       token_type_ids = token_type_ids,\n",
    "                       attention_mask = attention_mask,\n",
    "                       output_hidden_states = True)\n",
    "        \n",
    "        \n",
    "        # last hidden state of all tokens\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        \n",
    "        ################ Hidden States of each Transformer block\n",
    "        ## Index=0 -> initial hidden state as token embedding + position embedding + segment embedding\n",
    "        ## Index=13 -> last hidden state for each token in the sequence\n",
    "        ## \n",
    "        ## \"The ELMO authors suggest that lower levels encode syntax, while higher levels encode semantics.\"\n",
    "        # hidden_states = output.hidden_states\n",
    "        ################\n",
    "        \n",
    "        # hidden state of the first token e.g. classification token [CLS] or <s>\n",
    "        # not a good representation of the whole sequence for decoder-based models such as GPT2\n",
    "        cls_hidden_state = last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # TODO: average hidden state of each token for each layer as better representation\n",
    "        # TODO: consider earlier hidden states for syntax focused classification \n",
    "        return self.head(cls_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ad4f677-c307-45eb-8291-30034e2e9e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Program\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class RelationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        premise = self.data[\"premise\"].iloc[index]\n",
    "        claim = self.data[\"claim\"].iloc[index]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            premise,\n",
    "            claim,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "\n",
    "        if 'label' in self.data.columns:\n",
    "            \n",
    "            label = torch.tensor(0 if self.data[\"label\"].iloc[index] == \"Attack\" else 1, dtype=torch.int64)\n",
    "            \n",
    "            return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': label\n",
    "         }\n",
    "            \n",
    "        else:\n",
    "            return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c13b1cf2-804f-464d-aedd-df880fff935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import CONFIG\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from typing import List\n",
    "import pandas\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "class ClassificationModule(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        \n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        return self.model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "    \n",
    "    def step(self, batch, batch_idx, mode):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        token_type_ids = batch[\"token_type_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        #x, y = batch\n",
    "        logits = self.forward(input_ids, attention_mask, token_type_ids )\n",
    "\n",
    "        predictions = logits.argmax(dim = 1)\n",
    "        \n",
    "        loss = self.loss(logits, labels)\n",
    "        accuracy = self.accuracy(predictions, labels)\n",
    "\n",
    "        self.log(f'{mode}_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(f'{mode}_accuracy', accuracy, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx, 'train')\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx, 'val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx, 'test')\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n",
    "        \n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        token_type_ids = batch[\"token_type_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        logits = self(input_ids, attention_mask, token_type_ids)\n",
    "        predictions = logits.argmax(dim=-1)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.model.parameters(), lr=CONFIG.learning_rate)\n",
    "        lr_scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=CONFIG.warmup_steps,\n",
    "            num_training_steps=len(self.train_dataloader().dataset) // CONFIG.batch_size * CONFIG.epochs,\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": lr_scheduler, \"interval\": \"step\"}]\n",
    "\n",
    "    def create_data_loader(self, mode: str, shuffle=False):       \n",
    "        df = pd.read_pickle(\"../data/microtext_references.pickle\")\n",
    "        split = df[df['mode'] == mode]\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "            \n",
    "        return DataLoader(\n",
    "            RelationDataset(split, tokenizer),\n",
    "            batch_size = CONFIG.batch_size if mode == \"train\" else CONFIG.batch_size // 4,\n",
    "            shuffle=shuffle, num_workers = CONFIG.num_workers\n",
    "        )\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.create_data_loader(mode = \"train\", shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.create_data_loader(mode = \"validate\")\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.create_data_loader(mode = \"test\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "549729f2-5adc-4072-9ac5-4f0093a216f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, base_model_checkpoint, kepler_model_path, dropout=0.05, n_classes=2):\n",
    "        super(CombinedModel, self).__init__()\n",
    "\n",
    "        # Load the PyTorch trained base model\n",
    "        # Initialize base model class\n",
    "        self.base_model = BaseClassificationModel()\n",
    "        \n",
    "        # Initialize the ClassificationModule with the base model\n",
    "        self.base_module = ClassificationModule(self.base_model)\n",
    "        \n",
    "        # Load the checkpoint\n",
    "        checkpoint = torch.load(base_model_checkpoint)\n",
    "        \n",
    "        # Load the state dict into your base model\n",
    "        self.base_model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "        base_output_size = self.base_model.hidden_size # Fetch output size from base model\n",
    "        print(f\"base_output_size: {base_output_size}\")\n",
    "\n",
    "        # Load the pre-trained ERNIE model\n",
    "        kepler_config = RobertaConfig.from_pretrained(kepler_model_path)\n",
    "        self.kepler_model = RobertaForSequenceClassification.from_pretrained(kepler_model_path, config=kepler_config)\n",
    "        kepler_output_size = kepler_config.hidden_size  # Fetch output size from kepler model\n",
    "        print(f\"kepler_output_size: {kepler_output_size}\")\n",
    "\n",
    "        # Set the hidden size based on one of the models\n",
    "        #self.hidden_size = ernie_config.hidden_size\n",
    "        # Calculate the combined output size\n",
    "        combined_output_size = base_output_size + kepler_output_size\n",
    "        print(f\"combined_output_size: {combined_output_size}\")\n",
    "\n",
    "\n",
    "        # (combined) model classification head\n",
    "        #self.head = nn.Sequential(\n",
    "        #    nn.Dropout(dropout),\n",
    "        #    nn.Linear(770, 768), #16x1026 and 770x768\n",
    "        #    nn.Tanh(),\n",
    "        #    nn.Dropout(dropout),\n",
    "        #    nn.Linear(768, n_classes),\n",
    "        #)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(kepler_output_size + n_classes, base_output_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(base_output_size, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, base_model_inputs, kepler_model_inputs):\n",
    "        # Pass input through base model\n",
    "        base_output = self.base_model(**base_model_inputs)\n",
    "        #print(f\"base_output hidden_size: {base_output.hidden_size}\")\n",
    "    \n",
    "        # Pass input through ERNIE model\n",
    "        kepler_output = self.kepler_model(**kepler_model_inputs)\n",
    "        kepler_hidden_state = kepler_output.last_hidden_state[:, 0, :]\n",
    "        print(f\"kepler_hidden_state shape: {len(kepler_hidden_state)}\")\n",
    "\n",
    "        print(f'base_output shape: {base_output.shape}')\n",
    "        print(f'kepler_output shape: {kepler_output.shape}')\n",
    "\n",
    "        # Concatenate the base model's output and ERNIE model's cls hidden states\n",
    "        combined_output = torch.cat((base_output, kepler_output), dim=1)\n",
    "        print(f'combined_output shape: {combined_output.shape}')\n",
    "    \n",
    "        # Pass through final classification head\n",
    "        return self.head(combined_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1a8bc92e-bd8c-4cab-9f23-404a64f227a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_output_size: 768\n",
      "kepler_output_size: 768\n",
      "combined_output_size: 1536\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "base_model_checkpoint = \"../baseModel/trained_model/base_model.ckpt\"\n",
    "kepler_model_path = \"../notebooks/models/KEPLER/KEPLER_MICRO\"\n",
    "combined_model = CombinedModel(base_model_checkpoint, kepler_model_path)\n",
    "\n",
    "\n",
    "# Freeze the parameters\n",
    "for param in combined_model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in combined_model.kepler_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "combined_model = combined_model.to(device)\n",
    "\n",
    "# Tokenizers\n",
    "base_model_tokenizer = AutoTokenizer.from_pretrained(CONFIG.pretrained_model_name)\n",
    "kepler_model_tokenizer = RobertaTokenizer.from_pretrained( \"../notebooks/models/KEPLER/KEPLER_INPUT/\")\n",
    "\n",
    "# Example test data\n",
    "# Load the test dataset\n",
    "mapping = {'Attack': 0, 'Support': 1}\n",
    "df = pd.read_pickle(\"../data/microtext_references.pickle\")\n",
    "split = df[df['mode'] == 'test']\n",
    "true_labels = split['label'].map(mapping)\n",
    "#split = df[(df['mode'] == 'test') & (df['label'] != 'Rephrase')] #Kialo data set\n",
    "#true_labels = split['label'].map(mapping) #kialo data set\n",
    "\n",
    "# Prepare the datasets\n",
    "base_dataset = RelationDataset(split[['premise', 'claim']], base_model_tokenizer)\n",
    "kepler_dataset = RelationDataset(split[['premise', 'claim']], kepler_model_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1a2b9189-4e79-440a-8a9f-da76da5fb130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">17</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">14 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>kepler_batch = {k: v.to(device) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> k, v <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> kepler_batch.items()}                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">16 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Get model outputs</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>17 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>outputs = combined_model(base_batch, kepler_batch)                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">18 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">19 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Get the predictions from the outputs</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">20 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>predictions.extend(torch.argmax(outputs, dim=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>).tolist())                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/afigueroa/tfIntegration/.venv/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">56</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">53 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">54 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Pass input through ERNIE model</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">55 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>kepler_output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.kepler_model(**kepler_model_inputs)                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>56 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>kepler_hidden_state = kepler_output.hidden_states[-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>][:, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>, :]                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">57 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"kepler_hidden_state shape: {</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(kepler_hidden_state)<span style=\"color: #808000; text-decoration-color: #808000\">}\"</span>)                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">58 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">59 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f'base_output shape: {</span>base_output.shape<span style=\"color: #808000; text-decoration-color: #808000\">}'</span>)                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'NoneType'</span> object is not subscriptable\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m17\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m14 \u001b[0m\u001b[2m│   │   \u001b[0mkepler_batch = {k: v.to(device) \u001b[94mfor\u001b[0m k, v \u001b[95min\u001b[0m kepler_batch.items()}                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m15 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m16 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Get model outputs\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m17 \u001b[2m│   │   \u001b[0moutputs = combined_model(base_batch, kepler_batch)                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m18 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Get the predictions from the outputs\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m20 \u001b[0m\u001b[2m│   \u001b[0mpredictions.extend(torch.argmax(outputs, dim=\u001b[94m1\u001b[0m).tolist())                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/afigueroa/tfIntegration/.venv/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mforward\u001b[0m:\u001b[94m56\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m53 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m54 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Pass input through ERNIE model\u001b[0m                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m55 \u001b[0m\u001b[2m│   │   \u001b[0mkepler_output = \u001b[96mself\u001b[0m.kepler_model(**kepler_model_inputs)                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m56 \u001b[2m│   │   \u001b[0mkepler_hidden_state = kepler_output.hidden_states[-\u001b[94m1\u001b[0m][:, \u001b[94m0\u001b[0m, :]                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m57 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mkepler_hidden_state shape: \u001b[0m\u001b[33m{\u001b[0m\u001b[96mlen\u001b[0m(kepler_hidden_state)\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m58 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m59 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m'\u001b[0m\u001b[33mbase_output shape: \u001b[0m\u001b[33m{\u001b[0mbase_output.shape\u001b[33m}\u001b[0m\u001b[33m'\u001b[0m)                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0m\u001b[32m'NoneType'\u001b[0m object is not subscriptable\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare the dataloaders\n",
    "batch_size = 16  # Choose an appropriate batch size for your environment\n",
    "base_dataloader = DataLoader(base_dataset, batch_size=batch_size)\n",
    "kepler_dataloader = DataLoader(kepler_dataset, batch_size=batch_size)\n",
    "\n",
    "combined_model.eval()  # Set the model to evaluation mode\n",
    "predictions = []\n",
    "\n",
    "# Iterate over batches from both dataloaders\n",
    "for (base_batch, kepler_batch) in zip(base_dataloader, kepler_dataloader):\n",
    "    with torch.no_grad():\n",
    "        # Move batch to device\n",
    "        base_batch = {k: v.to(device) for k, v in base_batch.items()}\n",
    "        kepler_batch = {k: v.to(device) for k, v in kepler_batch.items()}\n",
    "\n",
    "        # Get model outputs\n",
    "        outputs = combined_model(base_batch, kepler_batch)\n",
    "    \n",
    "    # Get the predictions from the outputs\n",
    "    predictions.extend(torch.argmax(outputs, dim=1).tolist())\n",
    "\n",
    "#print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b88ca30-6b9a-496e-8e19-e33e905166b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
