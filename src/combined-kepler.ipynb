{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55ff9b7e-0cee-4cb1-8845-eebe90bbf13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "from transformers import RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from config import CONFIG\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e165024-96c3-45a1-b3b0-c7b00902a375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: generalize to [String] -> [Class] system \n",
    "# TODO: generalize forward pass\n",
    "# TODO: set parameter that enables cls token utilization or arbitrary hidden layer utilization\n",
    "# TODO: (MAYBE) generalize models to extend BASE, otherwise add \n",
    "from config import CONFIG\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from torch import nn\n",
    "\n",
    "class BaseClassificationModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, dropout = 0.05, n_classes = 2, injection = False):\n",
    "        super(BaseClassificationModel, self).__init__()\n",
    "        \n",
    "        \n",
    "        # model body\n",
    "        self.model = AutoModel.from_pretrained(CONFIG.pretrained_model_name)\n",
    "        \n",
    "        self.hidden_size = self.model.config.hidden_size #768\n",
    "        \n",
    "        # (standard) model classification head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.hidden_size, n_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # initialize weights in linear layers\n",
    "        self.init_weights(self.head)\n",
    "        \n",
    "        \n",
    "    def init_weights(self, module):\n",
    "        for layer in module:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.weight.data.normal_(mean = 0.0, std = 0.02)\n",
    "                if layer.bias is not None:\n",
    "                    layer.bias.data.zero_()\n",
    "                    \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        \n",
    "        output = self.model(input_ids = input_ids, \n",
    "                       token_type_ids = token_type_ids,\n",
    "                       attention_mask = attention_mask,\n",
    "                       output_hidden_states = True)\n",
    "        \n",
    "        # last hidden state of all tokens\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        \n",
    "        return last_hidden_state[:, 0, :]  # Returns the hidden state of the first token in the sequence\n",
    "\n",
    "    def predict(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        \n",
    "        cls_hidden_state = self.forward(input_ids, attention_mask, token_type_ids)\n",
    "        \n",
    "        return self.head(cls_hidden_state)  # Returns the predicted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b91ae20-cce7-4d52-98f6-5b3a87aa8e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Program\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class RelationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        premise = self.data[\"premise\"].iloc[index]\n",
    "        claim = self.data[\"claim\"].iloc[index]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            premise,\n",
    "            claim,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "\n",
    "        if 'label' in self.data.columns:\n",
    "            \n",
    "            label = torch.tensor(0 if self.data[\"label\"].iloc[index] == \"Attack\" else 1, dtype=torch.int64)\n",
    "            \n",
    "            return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': label\n",
    "         }\n",
    "            \n",
    "        else:\n",
    "            return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20f837c7-4262-4ab1-8192-bf1a3bfe685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import CONFIG\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from typing import List\n",
    "import pandas\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "class ClassificationModule(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        \n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        return self.model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "    \n",
    "    def step(self, batch, batch_idx, mode):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        token_type_ids = batch[\"token_type_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        #x, y = batch\n",
    "        #logits = self.forward(input_ids, attention_mask, token_type_ids )\n",
    "        logits = self.model.predict(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        predictions = logits.argmax(dim = 1)\n",
    "        \n",
    "        loss = self.loss(logits, labels)\n",
    "        accuracy = self.accuracy(predictions, labels)\n",
    "\n",
    "        self.log(f'{mode}_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(f'{mode}_accuracy', accuracy, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx, 'train')\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx, 'val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx, 'test')\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n",
    "        \n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        token_type_ids = batch[\"token_type_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        #logits = self(input_ids, attention_mask, token_type_ids)\n",
    "        logits = self.model.predict(input_ids, attention_mask, token_type_ids)\n",
    "        predictions = logits.argmax(dim=-1)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.model.parameters(), lr=CONFIG.learning_rate)\n",
    "        lr_scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=CONFIG.warmup_steps,\n",
    "            num_training_steps=len(self.train_dataloader().dataset) // CONFIG.batch_size * CONFIG.epochs,\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": lr_scheduler, \"interval\": \"step\"}]\n",
    "\n",
    "    def create_data_loader(self, mode: str, shuffle=False):       \n",
    "        df = pd.read_pickle(\"../data/microtext_references.pickle\")\n",
    "        split = df[df['mode'] == mode]\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(CONFIG.pretrained_model_name)\n",
    "            \n",
    "        return DataLoader(\n",
    "            RelationDataset(split, tokenizer),\n",
    "            batch_size = CONFIG.batch_size if mode == \"train\" else CONFIG.batch_size // 4,\n",
    "            shuffle=shuffle, num_workers = CONFIG.num_workers\n",
    "        )\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.create_data_loader(mode = \"train\", shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.create_data_loader(mode = \"validate\")\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.create_data_loader(mode = \"test\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e391cf72-17d2-42ac-8b87-3ffac463d9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, base_model_checkpoint, kepler_model_path, dropout=0.05, n_classes=2):\n",
    "        super(CombinedModel, self).__init__()\n",
    "\n",
    "        # Load the PyTorch trained base model\n",
    "        # Initialize base model class\n",
    "        self.base_model = BaseClassificationModel()\n",
    "        \n",
    "        # Initialize the ClassificationModule with the base model\n",
    "        self.base_module = ClassificationModule(self.base_model)\n",
    "        \n",
    "        # Load the checkpoint\n",
    "        checkpoint = torch.load(base_model_checkpoint)\n",
    "        \n",
    "        # Load the state dict into your base model\n",
    "        self.base_model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "        base_output_size = self.base_model.hidden_size # Fetch output size from base model\n",
    "        print(f\"base_output_size: {base_output_size}\")\n",
    "\n",
    "        # Load the pre-trained ERNIE model\n",
    "        kepler_config = AutoConfig.from_pretrained(kepler_model_path)\n",
    "        self.kepler_model = AutoModel.from_pretrained(kepler_model_path, config=kepler_config)\n",
    "        kepler_output_size = kepler_config.hidden_size  # Fetch output size from ernie model\n",
    "        print(f\"kepler_output_size: {kepler_output_size}\")\n",
    "\n",
    "\n",
    "        # Set the hidden size based on one of the models\n",
    "        #self.hidden_size = ernie_config.hidden_size\n",
    "        # Calculate the combined output size\n",
    "        combined_output_size = base_output_size + kepler_output_size\n",
    "        print(f\"combined_output_size: {combined_output_size}\")\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(combined_output_size, base_output_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(base_output_size, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, base_model_inputs, kepler_model_inputs):\n",
    "        # Pass input through base model\n",
    "        base_output = self.base_model(**base_model_inputs)\n",
    "        print(f\"base_output: {base_output.shape}\")\n",
    "        #print(f\"base_output: {base_output.last_hidden_state}\")\n",
    "\n",
    "        # Pass input through ERNIE model\n",
    "        kepler_output = self.kepler_model(**kepler_model_inputs)\n",
    "        kepler_hidden_state = kepler_output.last_hidden_state[:, 0, :]\n",
    "        print(f\"kepler_hidden_state shape: {kepler_hidden_state.shape}\")\n",
    "        #print(f\"ernie_hidden_state last_hidden_state: {len(ernie_output.last_hidden_state)}\")\n",
    "\n",
    "        # Concatenate the base model's output and ERNIE model's cls hidden states\n",
    "        combined_output = torch.cat((base_output, kepler_hidden_state), dim=1)\n",
    "        #print(f'combined_output shape: {combined_output.shape}')\n",
    "    \n",
    "        # Pass through final classification head\n",
    "        return self.head(combined_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89befeeb-9668-4ec5-9468-afbd7b27e4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_output_size: 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../notebooks/models/KEPLER/KEPLER_MICRO/ were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../notebooks/models/KEPLER/KEPLER_MICRO/ and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kepler_output_size: 768\n",
      "combined_output_size: 1536\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "base_model_checkpoint = \"./trained_model/base_model.ckpt\"\n",
    "kepler_model_path = \"../notebooks/models/KEPLER/KEPLER_MICRO/\"\n",
    "combined_model = CombinedModel(base_model_checkpoint, kepler_model_path)\n",
    "\n",
    "# Freeze the parameters\n",
    "for param in combined_model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in combined_model.kepler_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "combined_model = combined_model.to(device)\n",
    "\n",
    "# Tokenizers\n",
    "base_model_tokenizer = AutoTokenizer.from_pretrained(CONFIG.pretrained_model_name)\n",
    "kepler_model_tokenizer = RobertaTokenizer.from_pretrained(\"../notebooks/models/KEPLER/KEPLER_INPUT/\")\n",
    "\n",
    "# Example test data\n",
    "# Load the test dataset\n",
    "mapping = {'Attack': 0, 'Support': 1}\n",
    "df = pd.read_pickle(\"../data/microtext_references.pickle\")\n",
    "split = df[df['mode'] == 'test']\n",
    "true_labels = split['label'].map(mapping)\n",
    "#split = df[(df['mode'] == 'test') & (df['label'] != 'Rephrase')] #Kialo data set\n",
    "#true_labels = split['label'].map(mapping) #kialo data set\n",
    "\n",
    "# Prepare the datasets\n",
    "kepler_dataset = RelationDataset(split[['premise', 'claim']], kepler_model_tokenizer)\n",
    "base_dataset = RelationDataset(split[['premise', 'claim']], base_model_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "020e68dd-c079-4df3-98ac-76dfce24c636",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_output: torch.Size([16, 768])\n",
      "kepler_hidden_state shape: torch.Size([16, 768])\n",
      "base_output: torch.Size([16, 768])\n",
      "kepler_hidden_state shape: torch.Size([16, 768])\n",
      "base_output: torch.Size([16, 768])\n",
      "kepler_hidden_state shape: torch.Size([16, 768])\n",
      "base_output: torch.Size([1, 768])\n",
      "kepler_hidden_state shape: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# Prepare the dataloaders\n",
    "batch_size = 16  # Choose an appropriate batch size for your environment\n",
    "base_dataloader = DataLoader(base_dataset, batch_size=batch_size)\n",
    "kepler_dataloader = DataLoader(kepler_dataset, batch_size=batch_size)\n",
    "\n",
    "combined_model.eval()  # Set the model to evaluation mode\n",
    "predictions = []\n",
    "\n",
    "# Iterate over batches from both dataloaders\n",
    "for (base_batch, kepler_batch) in zip(base_dataloader, kepler_dataloader):\n",
    "    with torch.no_grad():\n",
    "        # Move batch to device\n",
    "        base_batch = {k: v.to(device) for k, v in base_batch.items()}\n",
    "        kepler_batch = {k: v.to(device) for k, v in kepler_batch.items()}\n",
    "\n",
    "        # Get model outputs\n",
    "        outputs = combined_model(base_batch, kepler_batch)\n",
    "    \n",
    "    # Get the predictions from the outputs\n",
    "    predictions.extend(torch.argmax(outputs, dim=1).tolist())\n",
    "\n",
    "#print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efd74a2a-27d4-4d7a-b091-996988aef864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      1.00      0.47        15\n",
      "           1       0.00      0.00      0.00        34\n",
      "\n",
      "    accuracy                           0.31        49\n",
      "   macro avg       0.15      0.50      0.23        49\n",
      "weighted avg       0.09      0.31      0.14        49\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afigueroa/tfIntegration/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/afigueroa/tfIntegration/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/afigueroa/tfIntegration/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print classification report\n",
    "report = classification_report(true_labels, predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c52332a-9f78-478f-b417-c59ab728a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, base_model_checkpoint, knowledge_model_path, dropout=0.05, n_classes=2):\n",
    "        super(CombinedModel, self).__init__()\n",
    "\n",
    "        # Load the PyTorch trained base model\n",
    "        # Initialize base model class\n",
    "        self.base_model = BaseClassificationModel()\n",
    "        \n",
    "        # Initialize the ClassificationModule with the base model\n",
    "        self.base_module = ClassificationModule(self.base_model)\n",
    "        \n",
    "        # Load the checkpoint\n",
    "        checkpoint = torch.load(base_model_checkpoint)\n",
    "        \n",
    "        # Load the state dict into your base model\n",
    "        self.base_model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "        base_output_size = self.base_model.hidden_size # Fetch output size from base model\n",
    "        print(f\"base_output_size: {base_output_size}\")\n",
    "\n",
    "        # Load the pre-trained knowledge model\n",
    "        knowledge_config = AutoConfig.from_pretrained(knowledge_model_path)\n",
    "        self.knowledge_model = AutoModel.from_pretrained(knowledge_model_path, config=knowledge_config)\n",
    "        knowledge_output_size = knowledge_config.hidden_size  # Fetch output size from knowledge model\n",
    "        print(f\"knowledge_output_size: {knowledge_output_size}\")\n",
    "\n",
    "\n",
    "        # Calculate the combined output size\n",
    "        combined_output_size = base_output_size + knowledge_output_size\n",
    "        print(f\"combined_output_size: {combined_output_size}\")\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(combined_output_size, base_output_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(base_output_size, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, base_model_inputs, knowledge_model_inputs):\n",
    "        # Pass input through base model\n",
    "        base_output = self.base_model(**base_model_inputs)\n",
    "        print(f\"base_output: {base_output.shape}\")\n",
    "        #print(f\"base_output: {base_output.last_hidden_state}\")\n",
    "\n",
    "        # Pass input through knowledge model\n",
    "        knowledge_output = self.knowledge_model(**knowledge_model_inputs)\n",
    "        knowledge_hidden_state = knowledge_output.last_hidden_state[:, 0, :]\n",
    "        print(f\"knowledge_hidden_state shape: {knowledge_hidden_state.shape}\")\n",
    "        #print(f\"knowledge_hidden_state last_hidden_state: {len(knowledge_output.last_hidden_state)}\")\n",
    "\n",
    "        # Concatenate the base model's output and knowledge model's cls hidden states\n",
    "        combined_output = torch.cat((base_output, knowledge_hidden_state), dim=1)\n",
    "        #print(f'combined_output shape: {combined_output.shape}')\n",
    "    \n",
    "        # Pass through final classification head\n",
    "        return self.head(combined_output)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def evaluate_combined_model(base_model_checkpoint, base_model_tokenizer, knowledge_model_path, knowledge_model_tokenizer):\n",
    "    combined_model = CombinedModel(base_model_checkpoint, knowledge_model_path)\n",
    "\n",
    "    # Freeze the parameters\n",
    "    for param in combined_model.base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in combined_model.knowledge_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    combined_model = combined_model.to(device)\n",
    "\n",
    "    # Load the test dataset\n",
    "    mapping = {'Attack': 0, 'Support': 1}\n",
    "    df = pd.read_pickle(CONFIG.data)\n",
    "    split = df[df['mode'] == 'test']\n",
    "    true_labels = split['label'].map(mapping)\n",
    "\n",
    "    # Prepare the datasets\n",
    "    knowledge_dataset = RelationDataset(split[['premise', 'claim']], knowledge_model_tokenizer)\n",
    "    base_dataset = RelationDataset(split[['premise', 'claim']], base_model_tokenizer)\n",
    "\n",
    "    # Prepare the dataloaders\n",
    "    batch_size = 16  # Choose an appropriate batch size for your environment\n",
    "    base_dataloader = DataLoader(base_dataset, batch_size=batch_size)\n",
    "    knowledge_dataloader = DataLoader(knowledge_dataset, batch_size=batch_size)\n",
    "\n",
    "    combined_model.eval()  # Set the model to evaluation mode\n",
    "    predictions = []\n",
    "\n",
    "    # Iterate over batches from both dataloaders\n",
    "    for (base_batch, knowledge_batch) in zip(base_dataloader, knowledge_dataloader):\n",
    "        with torch.no_grad():\n",
    "            # Move batch to device\n",
    "            base_batch = {k: v.to(device) for k, v in base_batch.items()}\n",
    "            knowledge_batch = {k: v.to(device) for k, v in knowledge_batch.items()}\n",
    "\n",
    "            # Get model outputs\n",
    "            outputs = combined_model(base_batch, knowledge_batch)\n",
    "        \n",
    "        # Get the predictions from the outputs\n",
    "        predictions.extend(torch.argmax(outputs, dim=1).tolist())\n",
    "\n",
    "    # Print classification report\n",
    "    report = classification_report(true_labels, predictions)\n",
    "    print(report)\n",
    "\n",
    "\n",
    "# Tokenizers\n",
    "base_model_tokenizer = AutoTokenizer.from_pretrained(CONFIG.pretrained_model_name) #BASE Tokenizer\n",
    "#knowledge_model_tokenizer = BertTokenizerFast.from_pretrained(CONFIG.ernie_pretrained_model_name) #ERNIE Tokenizer\n",
    "knowledge_model_tokenizer = RobertaTokenizer.from_pretrained(CONFIG.kepler_pretrained_input_model) #KEPLER Tokenizer\n",
    "\n",
    "#Checkpoints\n",
    "base_model_checkpoint = CONFIG.pretrained_output_model #BASE MODEL\n",
    "#knowledge_model_path = CONFIG.ernie_pretrained_output_model #ERNIE MODEL\n",
    "knowledge_model_path = CONFIG.kepler_pretrained_output_model #KEPLER MODEL\n",
    "\n",
    "evaluate_combined_model(base_model_checkpoint, base_model_tokenizer, knowledge_model_path, knowledge_model_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
