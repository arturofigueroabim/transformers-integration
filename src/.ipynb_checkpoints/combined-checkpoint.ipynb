{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55ff9b7e-0cee-4cb1-8845-eebe90bbf13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from config import CONFIG\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e165024-96c3-45a1-b3b0-c7b00902a375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: generalize to [String] -> [Class] system \n",
    "# TODO: generalize forward pass\n",
    "# TODO: set parameter that enables cls token utilization or arbitrary hidden layer utilization\n",
    "# TODO: (MAYBE) generalize models to extend BASE, otherwise add \n",
    "from config import CONFIG\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from torch import nn\n",
    "\n",
    "class BaseClassificationModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, dropout = 0.05, n_classes = 2, injection = False):\n",
    "        super(BaseClassificationModel, self).__init__()\n",
    "        \n",
    "        \n",
    "        # model body\n",
    "        self.model = AutoModel.from_pretrained(CONFIG.pretrained_model_name)\n",
    "        \n",
    "        self.hidden_size = self.model.config.hidden_size #768\n",
    "        \n",
    "        # (standard) model classification head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.hidden_size, n_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # initialize weights in linear layers\n",
    "        self.init_weights(self.head)\n",
    "        \n",
    "        \n",
    "    def init_weights(self, module):\n",
    "        for layer in module:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.weight.data.normal_(mean = 0.0, std = 0.02)\n",
    "                if layer.bias is not None:\n",
    "                    layer.bias.data.zero_()\n",
    "                    \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        \n",
    "        output = self.model(input_ids = input_ids, \n",
    "                       token_type_ids = token_type_ids,\n",
    "                       attention_mask = attention_mask,\n",
    "                       output_hidden_states = True)\n",
    "        \n",
    "        \n",
    "        # last hidden state of all tokens\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        \n",
    "        ################ Hidden States of each Transformer block\n",
    "        ## Index=0 -> initial hidden state as token embedding + position embedding + segment embedding\n",
    "        ## Index=13 -> last hidden state for each token in the sequence\n",
    "        ## \n",
    "        ## \"The ELMO authors suggest that lower levels encode syntax, while higher levels encode semantics.\"\n",
    "        # hidden_states = output.hidden_states\n",
    "        ################\n",
    "        \n",
    "        # hidden state of the first token e.g. classification token [CLS] or <s>\n",
    "        # not a good representation of the whole sequence for decoder-based models such as GPT2\n",
    "        cls_hidden_state = last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # TODO: average hidden state of each token for each layer as better representation\n",
    "        # TODO: consider earlier hidden states for syntax focused classification \n",
    "        return self.head(cls_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b91ae20-cce7-4d52-98f6-5b3a87aa8e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Program\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class RelationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        premise = self.data[\"premise\"].iloc[index]\n",
    "        claim = self.data[\"claim\"].iloc[index]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            premise,\n",
    "            claim,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "\n",
    "        if 'label' in self.data.columns:\n",
    "            \n",
    "            label = torch.tensor(0 if self.data[\"label\"].iloc[index] == \"Attack\" else 1, dtype=torch.int64)\n",
    "            \n",
    "            return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': label\n",
    "         }\n",
    "            \n",
    "        else:\n",
    "            return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20f837c7-4262-4ab1-8192-bf1a3bfe685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import CONFIG\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from typing import List\n",
    "import pandas\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "class ClassificationModule(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        \n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        return self.model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "    \n",
    "    def step(self, batch, batch_idx, mode):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        token_type_ids = batch[\"token_type_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        #x, y = batch\n",
    "        logits = self.forward(input_ids, attention_mask, token_type_ids )\n",
    "\n",
    "        predictions = logits.argmax(dim = 1)\n",
    "        \n",
    "        loss = self.loss(logits, labels)\n",
    "        accuracy = self.accuracy(predictions, labels)\n",
    "\n",
    "        self.log(f'{mode}_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(f'{mode}_accuracy', accuracy, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx, 'train')\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx, 'val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx, 'test')\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n",
    "        \n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        token_type_ids = batch[\"token_type_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        logits = self(input_ids, attention_mask, token_type_ids)\n",
    "        predictions = logits.argmax(dim=-1)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.model.parameters(), lr=CONFIG.learning_rate)\n",
    "        lr_scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=CONFIG.warmup_steps,\n",
    "            num_training_steps=len(self.train_dataloader().dataset) // CONFIG.batch_size * CONFIG.epochs,\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": lr_scheduler, \"interval\": \"step\"}]\n",
    "\n",
    "    def create_data_loader(self, mode: str, shuffle=False):       \n",
    "        df = pd.read_pickle(\"../data/microtext_references.pickle\")\n",
    "        split = df[df['mode'] == mode]\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "            \n",
    "        return DataLoader(\n",
    "            RelationDataset(split, tokenizer),\n",
    "            batch_size = CONFIG.batch_size if mode == \"train\" else CONFIG.batch_size // 4,\n",
    "            shuffle=shuffle, num_workers = CONFIG.num_workers\n",
    "        )\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.create_data_loader(mode = \"train\", shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.create_data_loader(mode = \"validate\")\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.create_data_loader(mode = \"test\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e391cf72-17d2-42ac-8b87-3ffac463d9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, base_model_checkpoint, ernie_model_path, dropout=0.05, n_classes=2):\n",
    "        super(CombinedModel, self).__init__()\n",
    "\n",
    "        # Load the PyTorch trained base model\n",
    "        # Initialize base model class\n",
    "        self.base_model = BaseClassificationModel()\n",
    "        \n",
    "        # Initialize the ClassificationModule with the base model\n",
    "        self.base_module = ClassificationModule(self.base_model)\n",
    "        \n",
    "        # Load the checkpoint\n",
    "        checkpoint = torch.load(base_model_checkpoint)\n",
    "        \n",
    "        # Load the state dict into your base model\n",
    "        self.base_model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "    \n",
    "\n",
    "        # Load the pre-trained ERNIE model\n",
    "        ernie_config = AutoConfig.from_pretrained(ernie_model_path)\n",
    "        self.ernie_model = AutoModel.from_pretrained(ernie_model_path, config=ernie_config)\n",
    "\n",
    "        # Set the hidden size based on one of the models\n",
    "        self.hidden_size = ernie_config.hidden_size\n",
    "        \n",
    "\n",
    "        # (combined) model classification head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(770, 768),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(768, n_classes),\n",
    "        )\n",
    "        #self.head = nn.Sequential(\n",
    "        #    nn.Linear(770, 768),\n",
    "        #   nn.ReLU(),\n",
    "        #     nn.Linear(768, n_classes),\n",
    "        #)\n",
    "\n",
    "    def forward(self, base_model_inputs, ernie_model_inputs):\n",
    "        # Pass input through base model\n",
    "        base_output = self.base_model(**base_model_inputs)\n",
    "        \n",
    "        # Pass input through ERNIE model\n",
    "        ernie_output = self.ernie_model(**ernie_model_inputs)\n",
    "\n",
    "        # Get the cls hidden state of the ERNIE model\n",
    "        ernie_hidden_state = ernie_output.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Concatenate the base model's output logits and ERNIE model's cls hidden states\n",
    "        combined_output = torch.cat((base_output, ernie_hidden_state), dim=1)\n",
    "\n",
    "        # Pass through final classification head\n",
    "        return self.head(combined_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52a48391-67f5-48de-b523-7b7e1d201dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at ../notebooks/models/ were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "base_model_checkpoint = \"./trained_model/base_model.ckpt\"\n",
    "ernie_model_path = \"../notebooks/models/\"\n",
    "combined_model = CombinedModel(base_model_checkpoint, ernie_model_path)\n",
    "\n",
    "# Freeze the parameters\n",
    "for param in combined_model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in combined_model.ernie_model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89befeeb-9668-4ec5-9468-afbd7b27e4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at ../notebooks/models/kialo/ were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "base_model_checkpoint = \"./trained_model/base_model_kialo.ckpt\"\n",
    "ernie_model_path = \"../notebooks/models/kialo/\"\n",
    "combined_model = CombinedModel(base_model_checkpoint, ernie_model_path)\n",
    "\n",
    "# Freeze the parameters\n",
    "for param in combined_model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in combined_model.ernie_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "combined_model = combined_model.to(device)\n",
    "\n",
    "# Tokenizers\n",
    "base_model_tokenizer = AutoTokenizer.from_pretrained(CONFIG.pretrained_model_name)\n",
    "ernie_model_tokenizer = BertTokenizerFast.from_pretrained('nghuyong/ernie-2.0-en')\n",
    "\n",
    "# Example test data\n",
    "# Load the test dataset\n",
    "mapping = {'Attack': 0, 'Support': 1}\n",
    "df = pd.read_pickle(\"../data/kialo_references.pickle\")\n",
    "#split = df[df['mode'] == 'test']\n",
    "#true_labels = split['label'].map(mapping)\n",
    "split = df[(df['mode'] == 'test') & (df['label'] != 'Rephrase')] #Kialo data set\n",
    "true_labels = split['label'].map(mapping) #kialo data set\n",
    "\n",
    "# Prepare the datasets\n",
    "ernie_dataset = RelationDataset(split[['premise', 'claim']], ernie_model_tokenizer)\n",
    "base_dataset = RelationDataset(split[['premise', 'claim']], base_model_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "020e68dd-c079-4df3-98ac-76dfce24c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataloaders\n",
    "batch_size = 16  # Choose an appropriate batch size for your environment\n",
    "base_dataloader = DataLoader(base_dataset, batch_size=batch_size)\n",
    "ernie_dataloader = DataLoader(ernie_dataset, batch_size=batch_size)\n",
    "\n",
    "combined_model.eval()  # Set the model to evaluation mode\n",
    "predictions = []\n",
    "\n",
    "# Iterate over batches from both dataloaders\n",
    "for (base_batch, ernie_batch) in zip(base_dataloader, ernie_dataloader):\n",
    "    with torch.no_grad():\n",
    "        # Move batch to device\n",
    "        base_batch = {k: v.to(device) for k, v in base_batch.items()}\n",
    "        ernie_batch = {k: v.to(device) for k, v in ernie_batch.items()}\n",
    "\n",
    "        # Get model outputs\n",
    "        outputs = combined_model(base_batch, ernie_batch)\n",
    "    \n",
    "    # Get the predictions from the outputs\n",
    "    predictions.extend(torch.argmax(outputs, dim=1).tolist())\n",
    "\n",
    "#print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efd74a2a-27d4-4d7a-b091-996988aef864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     13237\n",
      "           1       0.50      1.00      0.67     13480\n",
      "\n",
      "    accuracy                           0.50     26717\n",
      "   macro avg       0.25      0.50      0.34     26717\n",
      "weighted avg       0.25      0.50      0.34     26717\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afigueroa/tfIntegration/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/afigueroa/tfIntegration/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/afigueroa/tfIntegration/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print classification report\n",
    "report = classification_report(true_labels, predictions)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
